{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-25T16:02:10.094391Z",
     "iopub.status.busy": "2022-03-25T16:02:10.094135Z",
     "iopub.status.idle": "2022-03-25T16:02:13.897086Z",
     "shell.execute_reply": "2022-03-25T16:02:13.896316Z",
     "shell.execute_reply.started": "2022-03-25T16:02:10.094362Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T16:02:13.90058Z",
     "iopub.status.busy": "2022-03-25T16:02:13.900372Z",
     "iopub.status.idle": "2022-03-25T16:02:13.906439Z",
     "shell.execute_reply": "2022-03-25T16:02:13.905602Z",
     "shell.execute_reply.started": "2022-03-25T16:02:13.900557Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data paths\n",
    "dataset_path_1st = 'C:\\\\NASA\\\\ML\\\\1st_test'\n",
    "dataset_path_2nd = 'C:\\\\NASA\\\\ML\\\\2nd_test'\n",
    "dataset_path_3rd = 'C:\\\\NASA\\\\ML\\\\3rd_test\\\\4th_test\\\\txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T16:02:13.909934Z",
     "iopub.status.busy": "2022-03-25T16:02:13.909486Z",
     "iopub.status.idle": "2022-03-25T16:02:15.491212Z",
     "shell.execute_reply": "2022-03-25T16:02:15.490496Z",
     "shell.execute_reply.started": "2022-03-25T16:02:13.909888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test for the first file\n",
    "\n",
    "data_dir = 'C:\\\\NASA\\\\ML\\\\1st_test'\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    print(filename)\n",
    "    dataset=pd.read_csv(os.path.join(data_dir, filename), sep='\\t')\n",
    "    ax = dataset.plot(figsize = (24,6), title= \"Bearing Vibration\" , legend = True)\n",
    "    ax.set(xlabel=\"cycle(n)\", ylabel=\"vibration/acceleration(g)\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T16:02:15.493538Z",
     "iopub.status.busy": "2022-03-25T16:02:15.493264Z",
     "iopub.status.idle": "2022-03-25T16:02:15.605667Z",
     "shell.execute_reply": "2022-03-25T16:02:15.604957Z",
     "shell.execute_reply.started": "2022-03-25T16:02:15.493506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Root Mean Squared Sum\n",
    "def calculate_rms(df):\n",
    "    result = []\n",
    "    for col in df:\n",
    "        r = np.sqrt((df[col]**2).sum() / len(df[col]))\n",
    "        result.append(r)\n",
    "    return result\n",
    "\n",
    "# extract peak-to-peak features\n",
    "def calculate_p2p(df):\n",
    "    return np.array(df.max().abs() + df.min().abs())\n",
    "\n",
    "# extract shannon entropy (cut signals to 500 bins)\n",
    "def calculate_entropy(df):\n",
    "    ent = []\n",
    "    for col in df:\n",
    "        ent.append(entropy(pd.cut(df[col], 500).value_counts()))\n",
    "    return np.array(ent)\n",
    "# extract clearence factor\n",
    "def calculate_clearence(df):\n",
    "    result = []\n",
    "    for col in df:\n",
    "        r = ((np.sqrt(df[col].abs())).sum() / len(df[col]))**2\n",
    "        result.append(r)\n",
    "    return result\n",
    "\n",
    "def time_features(dataset_path, id_set=None):\n",
    "    time_features = ['mean','std','skew','kurtosis','entropy','rms','max','p2p', 'crest', 'clearence', 'shape', 'impulse']\n",
    "    cols1 = ['B1_x','B1_y','B2_x','B2_y','B3_x','B3_y','B4_x','B4_y']\n",
    "    cols2 = ['B1','B2','B3','B4']\n",
    "    \n",
    "    # initialize\n",
    "    if id_set == 1:\n",
    "        columns = [c+'_'+tf for c in cols1 for tf in time_features]\n",
    "        data = pd.DataFrame(columns=columns)\n",
    "    else:\n",
    "        columns = [c+'_'+tf for c in cols2 for tf in time_features]\n",
    "        data = pd.DataFrame(columns=columns)\n",
    "\n",
    "        \n",
    "        \n",
    "    for filename in os.listdir(dataset_path):\n",
    "        # read dataset\n",
    "        raw_data = pd.read_csv(os.path.join(dataset_path, filename), sep='\\t')\n",
    "        \n",
    "        # time features\n",
    "        mean_abs = np.array(raw_data.abs().mean())\n",
    "        std = np.array(raw_data.std())\n",
    "        skew = np.array(raw_data.skew())\n",
    "        kurtosis = np.array(raw_data.kurtosis())\n",
    "        entropy = calculate_entropy(raw_data)\n",
    "        rms = np.array(calculate_rms(raw_data))\n",
    "        max_abs = np.array(raw_data.abs().max())\n",
    "        p2p = calculate_p2p(raw_data)\n",
    "        crest = max_abs/rms\n",
    "        clearence = np.array(calculate_clearence(raw_data))\n",
    "        shape = rms / mean_abs\n",
    "        impulse = max_abs / mean_abs\n",
    "        \n",
    "        if id_set == 1:\n",
    "            mean_abs = pd.DataFrame(mean_abs.reshape(1,8), columns=[c+'_mean' for c in cols1])\n",
    "            std = pd.DataFrame(std.reshape(1,8), columns=[c+'_std' for c in cols1])\n",
    "            skew = pd.DataFrame(skew.reshape(1,8), columns=[c+'_skew' for c in cols1])\n",
    "            kurtosis = pd.DataFrame(kurtosis.reshape(1,8), columns=[c+'_kurtosis' for c in cols1])\n",
    "            entropy = pd.DataFrame(entropy.reshape(1,8), columns=[c+'_entropy' for c in cols1])\n",
    "            rms = pd.DataFrame(rms.reshape(1,8), columns=[c+'_rms' for c in cols1])\n",
    "            max_abs = pd.DataFrame(max_abs.reshape(1,8), columns=[c+'_max' for c in cols1])\n",
    "            p2p = pd.DataFrame(p2p.reshape(1,8), columns=[c+'_p2p' for c in cols1])\n",
    "            crest = pd.DataFrame(crest.reshape(1,8), columns=[c+'_crest' for c in cols1])\n",
    "            clearence = pd.DataFrame(clearence.reshape(1,8), columns=[c+'_clearence' for c in cols1])\n",
    "            shape = pd.DataFrame(shape.reshape(1,8), columns=[c+'_shape' for c in cols1])\n",
    "            impulse = pd.DataFrame(impulse.reshape(1,8), columns=[c+'_impulse' for c in cols1])\n",
    "            \n",
    "        else:\n",
    "            mean_abs = pd.DataFrame(mean_abs.reshape(1,4), columns=[c+'_mean' for c in cols2])\n",
    "            std = pd.DataFrame(std.reshape(1,4), columns=[c+'_std' for c in cols2])\n",
    "            skew = pd.DataFrame(skew.reshape(1,4), columns=[c+'_skew' for c in cols2])\n",
    "            kurtosis = pd.DataFrame(kurtosis.reshape(1,4), columns=[c+'_kurtosis' for c in cols2])\n",
    "            entropy = pd.DataFrame(entropy.reshape(1,4), columns=[c+'_entropy' for c in cols2])\n",
    "            rms = pd.DataFrame(rms.reshape(1,4), columns=[c+'_rms' for c in cols2])\n",
    "            max_abs = pd.DataFrame(max_abs.reshape(1,4), columns=[c+'_max' for c in cols2])\n",
    "            p2p = pd.DataFrame(p2p.reshape(1,4), columns=[c+'_p2p' for c in cols2])\n",
    "            crest = pd.DataFrame(crest.reshape(1,4), columns=[c+'_crest' for c in cols2])\n",
    "            clearence = pd.DataFrame(clearence.reshape(1,4), columns=[c+'_clearence' for c in cols2])\n",
    "            shape = pd.DataFrame(shape.reshape(1,4), columns=[c+'_shape' for c in cols2])\n",
    "            impulse = pd.DataFrame(impulse.reshape(1,4), columns=[c+'_impulse' for c in cols2])\n",
    "            \n",
    "        mean_abs.index = [filename]\n",
    "        std.index = [filename]\n",
    "        skew.index = [filename]\n",
    "        kurtosis.index = [filename]\n",
    "        entropy.index = [filename]\n",
    "        rms.index = [filename]\n",
    "        max_abs.index = [filename]\n",
    "        p2p.index = [filename]\n",
    "        crest.index = [filename]\n",
    "        clearence.index = [filename]\n",
    "        shape.index = [filename]\n",
    "        impulse.index = [filename] \n",
    "        \n",
    "        # concat\n",
    "        merge = pd.concat([mean_abs, std, skew, kurtosis, entropy, rms, max_abs, p2p,crest,clearence, shape, impulse], axis=1)\n",
    "        data = data.append(merge)\n",
    "        \n",
    "    if id_set == 1:\n",
    "        cols = [c+'_'+tf for c in cols1 for tf in time_features]\n",
    "        data = data[cols]\n",
    "    else:\n",
    "        cols = [c+'_'+tf for c in cols2 for tf in time_features]\n",
    "        data = data[cols]\n",
    "        \n",
    "    data.index = pd.to_datetime(data.index, format='%Y.%m.%d.%H.%M.%S')\n",
    "    data = data.sort_index()\n",
    "    return data                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = time_features(dataset_path_1st, id_set=1)\n",
    "set1.to_csv('set1_timefeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = pd.read_csv(\"./set1_timefeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set1.rename(columns={'Unnamed: 0':'time'})\n",
    "set1.set_index('time')\n",
    "set1.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
